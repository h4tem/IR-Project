{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\titouan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Simplified preprocess function that just handles lowercasing\n",
    "def preprocess(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Function to load data (assuming JSON and already imported json library)\n",
    "def load_data(filepath):\n",
    "    import json\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vsm(data):\n",
    "    doc_names = []\n",
    "    corpus = []\n",
    "    doc_passages = {}  # Stores passages by document for later retrieval\n",
    "\n",
    "    for doc_id, passages in data.items():\n",
    "        doc_passages[doc_id] = []\n",
    "        doc_text = []\n",
    "        for passage_id, text in passages.items():\n",
    "            processed_text = preprocess(text)\n",
    "            doc_names.append((doc_id, passage_id))\n",
    "            corpus.append(processed_text)\n",
    "            doc_passages[doc_id].append((passage_id, processed_text))\n",
    "            doc_text.append(processed_text)  # Collect all texts to a single document list\n",
    "        # Append the combined text of all passages in one document to the corpus\n",
    "        corpus.append(\" \".join(doc_text))\n",
    "        doc_names.append((doc_id, 'doc'))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, tfidf_matrix, doc_names, doc_passages\n",
    "\n",
    "def get_most_relevant_document(vectorizer, tfidf_matrix, doc_names, query):\n",
    "    tokenized_query = preprocess(query)\n",
    "    query_vector = vectorizer.transform([tokenized_query])\n",
    "    cos_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Only consider full document entries (identified by 'doc')\n",
    "    doc_scores = {}\n",
    "    for idx, (doc_id, type_id) in enumerate(doc_names):\n",
    "        if type_id == 'doc':\n",
    "            doc_scores[doc_id] = cos_similarities[idx]\n",
    "    most_relevant_doc = max(doc_scores, key=doc_scores.get)\n",
    "    return most_relevant_doc\n",
    "\n",
    "def get_top_passages_from_doc(doc_passages, doc_id, vectorizer, query, top_n=2):\n",
    "    passages = doc_passages[doc_id]\n",
    "    scores = []\n",
    "\n",
    "    for passage_id, text in passages:\n",
    "        passage_vector = vectorizer.transform([text])\n",
    "        score = cosine_similarity(passage_vector, vectorizer.transform([preprocess(query)])).flatten()[0]\n",
    "        scores.append((score, passage_id, text))\n",
    "\n",
    "    scores.sort(reverse=True, key=lambda x: x[0])\n",
    "    return scores[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(doc_id, top_passages):\n",
    "    print(f\"Most Relevant Document ID: {doc_id}\")\n",
    "    print(\"Top Passages:\")\n",
    "    for score, passage_id, text in top_passages:\n",
    "        print(f\"Passage ID: {passage_id}, Score: {score:.2f}\")\n",
    "        print(text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Document ID: 400\n",
      "Top Passages:\n",
      "Passage ID: 15, Score: 0.41\n",
      "there are 123 members of parliament in total. they are also alternatively called member of the national assembly. parliamentary elections are traditionally held every five years with no term limits imposed. the 25 provinces of cambodia are represented by the members of parliament in the national assembly. a constituency may have more than one mp, depending on the population. a member of parliament is a member of either of the two chambers of the parliament of the czech republic, although the term members of parliament of the czech republic is commonly referred to deputies of the parliament of the czech republic who are members of the lower house of the parliament, chamber of deputies.\n",
      "\n",
      "Passage ID: 0, Score: 0.39\n",
      "a member of parliament is the representative of the voters to a parliament. in many countries with bicameral parliaments, this category includes specifically members of the lower house, as upper houses often have a different title. members of parliament tend to form parliamentary groups with members of the same political party. the westminster system is a democratic parliamentary system of government modelled after the politics of the united kingdom. this term comes from the palace of westminster, the seat of the parliament of the united kingdom. a member of parliament is a member of the house of representatives, the lower house of the commonwealth parliament. members may use \"mp\" after their names; \"mhr\" is not used, although it was used as a post-nominal in the past.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    filepath = 'WikiPassageQA/document_passages.json'\n",
    "    data = load_data(filepath)\n",
    "    vectorizer, tfidf_matrix, doc_names, doc_passages = prepare_vsm(data)\n",
    "    query = \"What is the structure of Australiaâ€™s members of parliament?\"\n",
    "    most_relevant_doc = get_most_relevant_document(vectorizer, tfidf_matrix, doc_names, query)\n",
    "    top_passages = get_top_passages_from_doc(doc_passages, most_relevant_doc, vectorizer, query, top_n=2)\n",
    "    print_results(most_relevant_doc, top_passages)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
