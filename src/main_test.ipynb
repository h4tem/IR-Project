{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On convertit le fichier json contenant tous les passages en dict python\n",
    "\n",
    "file_path = 'WikiPassageQA\\document_passages.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    my_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour chaquer fichier (dev, test, train), on crée un dictionnaire ayant pr clés question id, doc id...\n",
    "\n",
    "def data(type): # type peut être dev, test ou train.\n",
    "\n",
    "    # Define the path to your text document\n",
    "    file_path = rf'WikiPassageQA\\{type}.txt'\n",
    "    \n",
    "    # Initialize an empty dictionary to hold your data\n",
    "    data_dict = {}\n",
    "    articles_with_ids = {}\n",
    "\n",
    "    # Open the text document for reading\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Skip the header line\n",
    "        next(file)\n",
    "        # Iterate over each line in the file\n",
    "        for line in file:\n",
    "            # Split the line into components based on tabs\n",
    "            parts = line.strip().split('\\t')\n",
    "            # Extract the individual components\n",
    "            qid, question, doc_id, doc_name, rel_passages = parts\n",
    "            # Convert the QID to an integer (if you want it as an integer)\n",
    "            qid = int(qid)\n",
    "            # Convert DocumentID to an integer (if needed)\n",
    "            doc_id = int(doc_id)\n",
    "            # Split 'RelevantPassages' into a list of integers (if they are always numbers)\n",
    "            rel_passages = [int(x) for x in rel_passages.split(',')]\n",
    "            # Populate the dictionary\n",
    "            data_dict[qid] = {\n",
    "                'Question': question,\n",
    "                'DocumentID': doc_id,\n",
    "                'DocumentName': doc_name,\n",
    "                'RelevantPassages': rel_passages\n",
    "            }\n",
    "            articles_with_ids[doc_id] = doc_name[:-5]\n",
    "    \n",
    "    return data_dict, articles_with_ids\n",
    "\n",
    "# At this point, data_dict contains your data structured as required\n",
    "# articles_with_ids : dict mapping document IDs to their Wikipedia article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise l'api de wikipedia pr segmenter chaque article en passages de 6 phrases. \n",
    "# PROBLEME : J'ai l'impression que c'est déjà ce qui est fait dans document_passages.json, useless donc ? \n",
    "#            Mais si c'était le cas, alors le bail des bi-grammes deviendrait complètement useless car les passage seraient EXACTEMENT identiques\n",
    "\n",
    "\n",
    "# Create a Wikipedia object with a specified user agent\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    user_agent='WikiPassageQAProject (hatem.mermoz@gmail.com)'\n",
    ")\n",
    "\n",
    "# Function to get Wikipedia page content\n",
    "def get_wiki_content(title):\n",
    "    page = wiki_wiki.page(title)\n",
    "    if page.exists():\n",
    "        return page.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to segment text into passages of six sentences\n",
    "def segment_text(text, sentences_per_passage=6):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    passages = [' '.join(sentences[i:i+sentences_per_passage]) for i in range(0, len(sentences), sentences_per_passage)]\n",
    "    return passages\n",
    "\n",
    "\n",
    "# Dictionary to hold article ID and their segmented text\n",
    "articles_dict = {}\n",
    "full_articles_dict = {} # Obligé de faire en 2 temps pcq sinon l'api pète un câble\n",
    "\n",
    "for type in [\"train\", \"dev\", \"train\"]:\n",
    "    articles_dict = {}\n",
    "    data_dict, articles_with_ids = data(type)\n",
    "\n",
    "    for doc_id, title in articles_with_ids.items():\n",
    "        content = get_wiki_content(title)\n",
    "        if content:\n",
    "            content = segment_text(content)\n",
    "            articles_dict[doc_id] = content \n",
    "\n",
    "    # On remplit le \"Vrai\" dico\n",
    "            \n",
    "    for key in articles_dict.keys():\n",
    "        full_articles_dict[key] = articles_dict[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXCEPTIONS : (fautes de frappe dans le doc texte :))))))))))))\n",
    "missing_keys = [347, 583, 517, 188, 208, 573, 228, 862]\n",
    "titles = [\"Encyclopædia_Britannica\", \"War_in_Afghanistan_(2001–2021)\",\n",
    "          \"Baháʼí_Faith\",\"Brussels\", \"Civil_rights_movement\", \"São_Paulo\", \"2007–2008_financial_crisis\"\n",
    "          , \"Eastern_Orthodox_Church\"]\n",
    "\n",
    "for key, title in zip(missing_keys,titles):\n",
    "    content = get_wiki_content(title)\n",
    "    full_articles_dict[key] = segment_text(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée un fichier json à partir des dico pour ne pas avoir à scrape à chaque fois #malinx \n",
    "\n",
    "# Define the filename where you want to store the data\n",
    "filename = 'data.json'\n",
    "\n",
    "# Open a file in write mode ('w') and use json.dump() to write the dictionary to the file\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(full_articles_dict, open(filename, 'w'), indent=4)  # The 'indent' parameter is optional but makes the file human-readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une fois que tout cqui est en haut a été run une fois, on utilsera plus que ça :\n",
    "\n",
    "file_path = 'data.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    wiki_articles_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(wiki_articles_dict) == len(my_dict))\n",
    "\n",
    "#SIUUUUUUUUUUUUUUUUU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
